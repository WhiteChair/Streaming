{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.115:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.115:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x50c9238a20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', \n",
    "                                                                  inferschema='true', \n",
    "                                                                  quote = '\"', \n",
    "                                                                  escape = '\"',\n",
    "                                                                 multiline = 'true',\n",
    "                                                                 ignoreTrailingWhiteSpace = 'true').load('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.na.drop(subset=[\"review_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as ff\n",
    "data = data.withColumn('text', ff.concat(ff.col('review_title'),ff.lit(' '),ff.col('review_text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['X', 'book_title', 'review_title', 'review_user', 'book_id', 'review_id', \n",
    "             'timestamp', 'review_text']\n",
    "data = data.select([column for column in data.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "import nltk\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 3333\n",
      "Test Dataset Count: 905\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = data.randomSplit([0.8, 0.2], seed = 12345)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordList = nltk.corpus.stopwords.words('english')\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stopwordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=20000, minDF=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"review_score\", outputCol = \"label\") \n",
    "labels_stars = label_stringIdx.fit(trainingData).labels # Save this levels to be able later to transform back\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|review_score|                text|               words|            filtered|            features|label|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|           1|*Eye roll* I firs...|[eye, roll, i, fi...|[eye, roll, first...|(415,[0,2,3,4,6,9...|  4.0|\n",
      "|           1|1 of the normal H...|[1, of, the, norm...|[1, normal, start...|(415,[0,16,32,42,...|  4.0|\n",
      "|           1|580 pages to just...|[580, pages, to, ...|[580, pages, quit...|(415,[0,1,2,3,4,5...|  4.0|\n",
      "|           1|A political hit j...|[a, political, hi...|[political, hit, ...|(415,[0,22,54,227...|  4.0|\n",
      "|           1|Amazing Read but ...|[amazing, read, b...|[amazing, read, d...|(415,[0,1,12,25,5...|  4.0|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineFit = pipeline.fit(trainingData)\n",
    "dataset = pipelineFit.transform(trainingData)\n",
    "#dataset.show(1, truncate = False)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|review_score|                text|               words|            filtered|            features|label|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|           1|A waste of money ...|[a, waste, of, mo...|[waste, money, ti...|(415,[5,10,56,375...|  4.0|\n",
      "|           1|Attn AMAZON Kindl...|[attn, amazon, ki...|[attn, amazon, ki...|(415,[0,11,116,13...|  4.0|\n",
      "|           1|Baid and Switch I...|[baid, and, switc...|[baid, switch, ha...|(415,[21,23,138,2...|  4.0|\n",
      "|           1|Boring ! I'm a hu...|[boring, i, m, a,...|[boring, huge, fa...|(415,[2,22,144,30...|  4.0|\n",
      "|           1|Boring Great plot...|[boring, great, p...|[boring, great, p...|(415,[4,25,63,230...|  4.0|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = pipelineFit.transform(testData)\n",
    "test_dataset.show(1, truncate = False)\n",
    "test_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "pipeline_tfidf = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|review_score|                text|               words|            filtered|         rawFeatures|            features|label|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|           1|*Eye roll* I firs...|[eye, roll, i, fi...|[eye, roll, first...|(20000,[456,1305,...|(20000,[456,1305,...|  4.0|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineFit_tfidf = pipeline_tfidf.fit(trainingData)\n",
    "dataset_tfidf = pipelineFit_tfidf.transform(trainingData)\n",
    "dataset_tfidf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|review_score|                text|               words|            filtered|         rawFeatures|            features|label|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|           1|A waste of money ...|[a, waste, of, mo...|[waste, money, ti...|(20000,[437,450,1...|(20000,[437,450,1...|  4.0|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset_tfidf = pipelineFit_tfidf.transform(testData)\n",
    "test_dataset_tfidf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------------------+------------+--------------+\n",
      "|                          text|                   probability|review_score|predictedScore|\n",
      "+------------------------------+------------------------------+------------+--------------+\n",
      "|The best book you will ever...|[0.9999962814618197,3.50321...|           5|             5|\n",
      "|What a POWERFUL story!! Unp...|[0.9999770160552786,2.29834...|           5|             5|\n",
      "|Beautifully written with el...|[0.9999733020487866,2.50650...|           4|             5|\n",
      "|Amazing, beautiful, accessi...|[0.9999701132582072,2.15653...|           5|             5|\n",
      "|Bravo, Mr. Towles! After re...|[0.9999586010516772,1.89201...|           5|             5|\n",
      "|exceptional novel - one of ...|[0.9999424971293328,5.64261...|           5|             5|\n",
      "|Rachel Provides Easy Steps ...|[0.999927561543873,6.041262...|           5|             5|\n",
      "|Anyone ready to change thei...|[0.9999228354249022,7.71509...|           5|             5|\n",
      "|A Gentleman in Moscow is an...|[0.9999224662957438,7.75334...|           5|             5|\n",
      "|5 stars aren’t Enough! What...|[0.9999204340077175,7.88800...|           5|             5|\n",
      "+------------------------------+------------------------------+------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedScore\", labels = labels_stars)\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "\n",
    "model = nb.fit(dataset)\n",
    "predictions = model.transform(test_dataset)\n",
    "predictions = labelConverter.transform(predictions) # Transform labels\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|Beautifully done and down to earth I ...|[1.0,1.0951071313748669E-16,5.0335173...|           5|             5|\n",
      "|SO Twisty Twisted “I want my kids to ...|[1.0,6.689446406497144E-17,2.61317220...|           5|             5|\n",
      "|Engrossing story! During the first pa...|[1.0,3.252631500218192E-17,7.17720871...|           5|             5|\n",
      "|SLOW BUILD TO THE TOP OF THE ROLLER C...|[1.0,3.044315421944498E-17,3.72133604...|           5|             5|\n",
      "|On the edge of my seat The author rea...|[1.0,2.4405740959251534E-17,2.8124606...|           5|             5|\n",
      "|Great book! This has been the best bo...|[1.0,1.6857201275336582E-17,4.7220161...|           5|             5|\n",
      "|Can I Give It 10 Stars? What an amazi...|[1.0,1.2134401333156454E-17,1.3185953...|           5|             5|\n",
      "|King at his absolute best. This book ...|[1.0,8.731671491790918E-18,1.16207237...|           5|             5|\n",
      "|Another amazing Black Dagger Brotherh...|[1.0,6.608692984950224E-18,4.17227784...|           5|             5|\n",
      "|A Must Read This is one of the best b...|[1.0,4.164189697349588E-18,4.47423826...|           5|             5|\n",
      "|An Elegant Read - Highly recommended ...|[1.0,3.589723878756446E-18,1.41322591...|           5|             5|\n",
      "|Choose Your Own Adventure! This book ...|[1.0,1.7412393227994232E-18,8.7725755...|           5|             5|\n",
      "|Excellent book This was a well writte...|[1.0,1.6684852576570633E-18,1.2580050...|           5|             5|\n",
      "|I couldn't put it down! Talk about a ...|[1.0,1.5716867169963069E-18,2.2419950...|           5|             5|\n",
      "|Gripping, Consuming This book, with S...|[1.0,1.3784849694631732E-18,7.1969513...|           5|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr_tfidf = LogisticRegression(maxIter=20, regParam=0, elasticNetParam=0)\n",
    "\n",
    "lrModel_tfidf = lr_tfidf.fit(dataset_tfidf)\n",
    "predictions_tfidf = lrModel_tfidf.transform(test_dataset_tfidf)\n",
    "predictions_tfidf = labelConverter.transform(predictions_tfidf) # Transform labels\n",
    "\n",
    "predictions_tfidf.filter(predictions_tfidf['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7441898341686789"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator_tfidf = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator_tfidf.evaluate(predictions_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_final= Pipeline(stages=[regexTokenizer, stopwordsRemover, label_stringIdx, nb])\n",
    "pipelineFit1=pipeline_final.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_final2= Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx, lr_tfidf])\n",
    "pipelineFit2=pipeline_final2.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-4dda9bdca0c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#pipelineFit1.save['pipelinemodel']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpipelineFit2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pipelinemodel2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#pipelineFit1.save['pipelinemodel']\n",
    "pipelineFit2.save['pipelinemodel2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "\n",
    "#def predict(df):\n",
    "    # Replace this with something smarter\n",
    "    \n",
    "#predict_udf = udf(predictions_tfidf, IntegerType())\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    # Utilize our predict function\n",
    "    df_withpreds = df.withColumn(\"pred\", predict_udf(struct([df[x] for x in df.columns])))\n",
    "    df_withpreds.show()\n",
    "    \n",
    "    #df_withpreds = df.withColumn(\"pred\", model_tfidf.transform(struct([df[x] for x in df.columns])))\n",
    "    #df_withpreds.show()\n",
    "    \n",
    "    # Normally, you wouldn't use a Python function to predict\n",
    "    # But an MLlib model you've built and saved with Spark\n",
    "    \n",
    "    # In this case, you need to prevent loading your model in every call to \"process\" as follows:\n",
    "    \n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] =   PipelineFit1.load('naivebayes') # Replace this with:    [...].load('my_logistic_regression')\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "        #pipelineFit=PipelineFit1.load('naivebayes')\n",
    "        #predictions= pipelineFit.transform(trainingdata)\n",
    "        \n",
    "    # Predict using the model: \n",
    "    df_result = globals()['model_tfidf'].transform(df)\n",
    "    df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "\n",
    "#def predict(df):\n",
    "    # Replace this with something smarter\n",
    "    \n",
    "#predict_udf = udf(predictions_tfidf, IntegerType())\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    # Utilize our predict function\n",
    "    df_withpreds = df.withColumn(\"pred\", predict_udf(struct([df[x] for x in df.columns])))\n",
    "    df_withpreds.show()\n",
    "    \n",
    "    #df_withpreds = df.withColumn(\"pred\", model_tfidf.transform(struct([df[x] for x in df.columns])))\n",
    "    #df_withpreds.show()\n",
    "    \n",
    "    # Normally, you wouldn't use a Python function to predict\n",
    "    # But an MLlib model you've built and saved with Spark\n",
    "    \n",
    "    # In this case, you need to prevent loading your model in every call to \"process\" as follows:\n",
    "    \n",
    "    # Load in the model if not yet loaded:\n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] =   PipelineFit2.load('logisticregtfidf') # Replace this with:    [...].load('my_logistic_regression')\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "        #pipelineFit=PipelineFit2.load('logisticregtfidf')\n",
    "        #predictions= pipelineFit.transform(trainingdata)\n",
    "        \n",
    "    # Predict using the model: \n",
    "    df_result = globals()['model_tfidf'].transform(df)\n",
    "    df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2019-06-01 12:48:10 =========\n",
      "+----------+--------------------+--------------+------------+--------------------+--------------------+-----------+----------+\n",
      "|   book_id|          book_title|     review_id|review_score|         review_text|        review_title|review_user| timestamp|\n",
      "+----------+--------------------+--------------+------------+--------------------+--------------------+-----------+----------+\n",
      "|1328557472|No Crumbs Left: W...|R35EMLV2B1CEPZ|           5|In this love lett...|              Superb|    Jenn N.|1559385539|\n",
      "|1982102314|           Elevation|R3BCWAFPY05KH5|           5|Lovely novelette....|Good Old Stephen ...| R. Vincent|1559385675|\n",
      "+----------+--------------------+--------------+------------+--------------------+--------------------+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Gianina\\Anaconda3\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-2-19fbf29765bb>\", line 9, in run\n",
      "    ssc.awaitTermination()\n",
      "  File \"C:\\Users\\Gianina\\Documents\\MSc\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\pyspark\\streaming\\context.py\", line 192, in awaitTermination\n",
      "    self._jssc.awaitTermination()\n",
      "  File \"C:\\Users\\Gianina\\Documents\\MSc\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"C:\\Users\\Gianina\\Documents\\MSc\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"C:\\Users\\Gianina\\Documents\\MSc\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1434.awaitTermination.\n",
      ": org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Gianina\\Documents\\MSc\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\pyspark\\streaming\\util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"<ipython-input-32-8868b7373e8d>\", line 18, in process\n",
      "    df_withpreds = df.withColumn(\"pred\", predict_udf(struct([df[x] for x in df.columns])))\n",
      "NameError: name 'predict_udf' is not defined\n",
      "\r\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\r\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\r\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\r\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\r\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\r\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\r\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\r\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)\r\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\r\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\r\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\r\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\r\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\r\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)\r\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\r\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)\r\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\r\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n",
      "\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2019-06-01 12:48:20 =========\n",
      "+----------+--------------------+--------------+------------+--------------------+--------------------+-----------+----------+\n",
      "|   book_id|          book_title|     review_id|review_score|         review_text|        review_title|review_user| timestamp|\n",
      "+----------+--------------------+--------------+------------+--------------------+--------------------+-----------+----------+\n",
      "|1476773092|Unfreedom of the ...| RLH9V0K65R60T|           5|Mark is a great A...|        A must read!|Serenity...|1559385829|\n",
      "|1982102314|           Elevation|R33M9K1SE0GEFG|           5|Lovely novelette....|Good Old Stephen ...| R. Vincent|1559386080|\n",
      "+----------+--------------------+--------------+------------+--------------------+--------------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
