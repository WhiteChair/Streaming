{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', \n",
    "                                                                  inferschema='true', \n",
    "                                                                  quote = '\"', \n",
    "                                                                  escape = '\"',\n",
    "                                                                 multiline = 'true',\n",
    "                                                                 ignoreTrailingWhiteSpace = 'true').load('Data\\\\data.csv')\n",
    "\n",
    "# There were some problems reading the data, here I found the solutions\n",
    "# https://stackoverflow.com/questions/40413526/reading-csv-files-with-quoted-fields-containing-embedded-commas\n",
    "#https://stackoverflow.com/questions/50477857/spark-fails-to-read-csv-when-last-column-name-contains-spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------+----------+--------------+----------+--------------------+------------+\n",
      "|  X|          book_title|        review_title|   review_user|   book_id|     review_id| timestamp|         review_text|review_score|\n",
      "+---+--------------------+--------------------+--------------+----------+--------------+----------+--------------------+------------+\n",
      "|  1|A Gentleman in Mo...|Russian aristocra...|    Kansabelle|0143110438|R2UFCQ9WES7VFH|1555241537|A great read. In ...|           4|\n",
      "|  2|A Gentleman in Mo...|Knowing nothing a...|  D.P. McHenry|0143110438|R24B1HA9J9I99G|1555241542|Great story, well...|           4|\n",
      "|  3|Pet Sematary: A N...|One of King's fin...|Gordon Hoffman|198211598X|R1P137WFADSBYR|1555241649|Only the second n...|           4|\n",
      "|  4|Less (Winner of t...|     Not my favorite|     R. Zocher|0316316121|R35533AKR5CBNS|1555242044|This book is t wh...|           4|\n",
      "|  5|         Supermarket|      AMAZING, BOBBY|    D. Mahoney|1982127139|R1D6LXSDR0CRMN|1555242169|As someone who ha...|           4|\n",
      "+---+--------------------+--------------------+--------------+----------+--------------+----------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop_list = ['Dates', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "#data = data.select([column for column in data.columns if column not in drop_list])\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- X: integer (nullable = true)\n",
      " |-- book_title: string (nullable = true)\n",
      " |-- review_title: string (nullable = true)\n",
      " |-- review_user: string (nullable = true)\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- review_score: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 3236|\n",
      "|           4|  598|\n",
      "|           3|  187|\n",
      "|           1|  110|\n",
      "|           2|  108|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "data.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4239"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+-----------+-------+---------+---------+-----------+------------+\n",
      "|  X|book_title|review_title|review_user|book_id|review_id|timestamp|review_text|review_score|\n",
      "+---+----------+------------+-----------+-------+---------+---------+-----------+------------+\n",
      "+---+----------+------------+-----------+-------+---------+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data.filter(\"review_score is NULL\").show() # No nulls anymore\n",
    "#data.filter(\"X is NULL\").show() \n",
    "#data.filter(\"book_title is NULL\").show()\n",
    "#data.filter(\"review_title is NULL\").show()\n",
    "#data.filter(\"review_user is NULL\").show()\n",
    "#data.filter(\"book_id is NULL\").show()\n",
    "#data.filter(\"review_id is NULL\").show()\n",
    "#data.filter(\"timestamp is NULL\").show()\n",
    "data.filter(\"review_text is NULL\").show() # There is one null\n",
    "#data.filter(\"review_score is NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observation where review_text is null\n",
    "data = data.na.drop(subset=[\"review_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4238"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count() # Removed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Now the modelling pipeline starts</h3>\n",
    "I got it from: https://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"review_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------+----------+--------------+----------+--------------------+------------+--------------------+--------------------+--------------------+-----+\n",
      "|  X|          book_title|        review_title|   review_user|   book_id|     review_id| timestamp|         review_text|review_score|               words|            filtered|            features|label|\n",
      "+---+--------------------+--------------------+--------------+----------+--------------+----------+--------------------+------------+--------------------+--------------------+--------------------+-----+\n",
      "|  1|A Gentleman in Mo...|Russian aristocra...|    Kansabelle|0143110438|R2UFCQ9WES7VFH|1555241537|A great read. In ...|           4|[a, great, read, ...|[a, great, read, ...|(4185,[1,2,3,4,7,...|  1.0|\n",
      "|  2|A Gentleman in Mo...|Knowing nothing a...|  D.P. McHenry|0143110438|R24B1HA9J9I99G|1555241542|Great story, well...|           4|[great, story, we...|[great, story, we...|(4185,[11,28,48,5...|  1.0|\n",
      "|  3|Pet Sematary: A N...|One of King's fin...|Gordon Hoffman|198211598X|R1P137WFADSBYR|1555241649|Only the second n...|           4|[only, the, secon...|[only, second, no...|(4185,[0,3,4,5,7,...|  1.0|\n",
      "|  4|Less (Winner of t...|     Not my favorite|     R. Zocher|0316316121|R35533AKR5CBNS|1555242044|This book is t wh...|           4|[this, book, is, ...|[this, book, is, ...|(4185,[1,3,6,7,8,...|  1.0|\n",
      "|  5|         Supermarket|      AMAZING, BOBBY|    D. Mahoney|1982127139|R1D6LXSDR0CRMN|1555242169|As someone who ha...|           4|[as, someone, who...|[as, someone, who...|(4185,[0,3,5,6,7,...|  1.0|\n",
      "+---+--------------------+--------------------+--------------+----------+--------------+----------+--------------------+------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"review_score\", outputCol = \"label\") \n",
    "# In this line above, the encoding of the target variable changed:\n",
    "# 5 is now 0\n",
    "# 4 is now 1\n",
    "# 3 is now 2\n",
    "# 1 is now 3\n",
    "# 2 is now 4\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0| 3235|\n",
      "|  1.0|  598|\n",
      "|  2.0|  187|\n",
      "|  3.0|  110|\n",
      "|  4.0|  108|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show how the encoding changed\n",
    "dataset.groupBy(\"label\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 3333\n",
      "Test Dataset Count: 905\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 12345)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic regression using count vector features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "|                   review_text|review_score|                   probability|label|prediction|\n",
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "|A Gentleman in Moscow was a...|           5|[0.997549675457473,0.002126...|  0.0|       0.0|\n",
      "|When We Left Cuba by Chanel...|           5|[0.9941138638357997,0.00423...|  0.0|       0.0|\n",
      "|Reviewed by Sharon Thérèse ...|           5|[0.9941014451392723,0.00107...|  0.0|       0.0|\n",
      "|If you're in need of cake, ...|           5|[0.9935674806273247,0.00349...|  0.0|       0.0|\n",
      "|I've been following Stella ...|           5|[0.9924354394575713,0.00361...|  0.0|       0.0|\n",
      "|If you don't find yourself ...|           5|[0.992276189543571,0.005088...|  0.0|       0.0|\n",
      "|Received my copy yesterday,...|           5|[0.9890898400637897,0.00521...|  0.0|       0.0|\n",
      "|I believe this book will be...|           5|[0.9874319617637527,0.00838...|  0.0|       0.0|\n",
      "|I’LL BE GONE IN THE DARK: O...|           5|[0.9865796791029282,0.01032...|  0.0|       0.0|\n",
      "|The title is something of a...|           5|[0.9850693483699189,0.01367...|  0.0|       0.0|\n",
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"review_text\",\"review_score\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(X=5, book_title='Supermarket', review_title='AMAZING, BOBBY', review_user='D. Mahoney', book_id='1982127139', review_id='R1D6LXSDR0CRMN', timestamp=1555242169, review_text=\"As someone who hates reading and never really got into any books in school, this book is a masterpiece. Don't think, just buy it asap. You won't regret it.\", review_score=4, words=['as', 'someone', 'who', 'hates', 'reading', 'and', 'never', 'really', 'got', 'into', 'any', 'books', 'in', 'school', 'this', 'book', 'is', 'a', 'masterpiece', 'don', 't', 'think', 'just', 'buy', 'it', 'asap', 'you', 'won', 't', 'regret', 'it'], filtered=['as', 'someone', 'who', 'hates', 'reading', 'and', 'never', 'really', 'got', 'into', 'any', 'books', 'in', 'school', 'this', 'book', 'is', 'a', 'masterpiece', 'don', 'think', 'just', 'buy', 'it', 'asap', 'you', 'won', 'regret', 'it'], features=SparseVector(4185, {0: 1.0, 3: 1.0, 5: 2.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0, 14: 1.0, 18: 1.0, 43: 1.0, 50: 1.0, 59: 1.0, 65: 1.0, 69: 1.0, 70: 1.0, 110: 1.0, 127: 1.0, 128: 1.0, 145: 1.0, 211: 1.0, 303: 1.0, 326: 1.0, 397: 1.0, 752: 1.0, 1031: 1.0, 1593: 1.0, 3460: 1.0}), label=1.0, rawPrediction=DenseVector([2.6937, -0.0592, -0.6307, -0.6959, -1.3079]), probability=DenseVector([0.8682, 0.0553, 0.0313, 0.0293, 0.0159]), prediction=0.0)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7115821172137888"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions) # This is the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression using TF-IDF Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "|                   review_text|review_score|                   probability|label|prediction|\n",
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "|A Gentleman in Moscow was a...|           5|[0.9977097070609728,0.00180...|  0.0|       0.0|\n",
      "|We humans like to think we ...|           5|[0.9962329444391925,0.00187...|  0.0|       0.0|\n",
      "|When We Left Cuba by Chanel...|           5|[0.9944781428791521,0.00446...|  0.0|       0.0|\n",
      "|If you're in need of cake, ...|           5|[0.9942286540178279,0.00231...|  0.0|       0.0|\n",
      "|Burnout by Emily (PhD) & Am...|           5|[0.9939724071927272,0.00186...|  0.0|       0.0|\n",
      "|What's great:* Expanding th...|           4|[0.9922029187079426,0.00246...|  1.0|       0.0|\n",
      "|Rachel Hollis did, yet agai...|           5|[0.989122019507554,0.006504...|  0.0|       0.0|\n",
      "|I really enjoyed this book ...|           5|[0.989101589600112,0.008920...|  0.0|       0.0|\n",
      "|When a member of our family...|           5|[0.9881662730314872,0.00734...|  0.0|       0.0|\n",
      "|A Gentleman in Moscow is a ...|           5|[0.9869434740584966,0.00747...|  0.0|       0.0|\n",
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 12345)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"review_text\",\"review_score\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7117679410509941"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-validation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7221962047615811"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 12345)\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "predictions = cvModel.transform(testData)\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Naive Bayes </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "|                   review_text|review_score|                   probability|label|prediction|\n",
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "|When We Left Cuba by Chanel...|           5|[1.0,6.594524434807532E-20,...|  0.0|       0.0|\n",
      "|I really enjoyed this book ...|           5|[1.0,3.4941127111910437E-20...|  0.0|       0.0|\n",
      "|We humans like to think we ...|           5|[1.0,1.5877606440176648E-24...|  0.0|       0.0|\n",
      "|I love reading about health...|           4|[0.9999999999999998,2.23135...|  1.0|       0.0|\n",
      "|Let's be honest. There is N...|           5|[0.9999999999999993,6.58594...|  0.0|       0.0|\n",
      "|The voice acting in this bo...|           5|[0.9999999999999984,1.51433...|  0.0|       0.0|\n",
      "|I knew right away when I sa...|           5|[0.9999999999999918,8.32501...|  0.0|       0.0|\n",
      "|If you enjoyed the historic...|           4|[0.9999999999997786,2.21488...|  1.0|       0.0|\n",
      "|Rachel Hollis did, yet agai...|           5|[0.9999999999997571,2.42952...|  0.0|       0.0|\n",
      "|Received my copy yesterday,...|           5|[0.9999999999997533,2.46605...|  0.0|       0.0|\n",
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "model = nb.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"review_text\",\"review_score\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7299769820931572"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forest</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "|                   review_text|review_score|                   probability|label|prediction|\n",
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "|Loved this story. Beautiful...|           4|[0.7836402391282297,0.12735...|  1.0|       0.0|\n",
      "|Very informative about Keto...|           5|[0.7836402391282297,0.12735...|  0.0|       0.0|\n",
      "|One of the most enjoyable b...|           5|[0.7836402391282297,0.12735...|  0.0|       0.0|\n",
      "|                Wonderful book|           4|[0.7836402391282297,0.12735...|  1.0|       0.0|\n",
      "|Love the easy to follow roa...|           5|[0.7830368095617698,0.12784...|  0.0|       0.0|\n",
      "|Poignant and very satisfyin...|           5|[0.7828176595848821,0.12763...|  0.0|       0.0|\n",
      "|Wonderful book. Beautiful p...|           5|[0.7827614778707869,0.12824...|  0.0|       0.0|\n",
      "|Well written,wonderful char...|           5|[0.7827614778707869,0.12824...|  0.0|       0.0|\n",
      "|This novel is historical, p...|           5|[0.78218092057711,0.1280738...|  0.0|       0.0|\n",
      "|Once again Dr. Joe has gift...|           5|[0.7820820230562446,0.12871...|  0.0|       0.0|\n",
      "+------------------------------+------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"review_text\",\"review_score\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6777035552331478"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.66666599999999,
   "position": {
    "height": "40px",
    "left": "1010px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
