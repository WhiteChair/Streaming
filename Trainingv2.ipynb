{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', \n",
    "                                                                  inferschema='true', \n",
    "                                                                  quote = '\"', \n",
    "                                                                  escape = '\"',\n",
    "                                                                 multiline = 'true',\n",
    "                                                                 ignoreTrailingWhiteSpace = 'true').load('data.csv')\n",
    "\n",
    "# There were some problems reading the data, here I found the solutions\n",
    "# https://stackoverflow.com/questions/40413526/reading-csv-files-with-quoted-fields-containing-embedded-commas\n",
    "#https://stackoverflow.com/questions/50477857/spark-fails-to-read-csv-when-last-column-name-contains-spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>book_title</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_user</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A Gentleman in Moscow: A Novel</td>\n",
       "      <td>Russian aristocracy following the Russian Revo...</td>\n",
       "      <td>Kansabelle</td>\n",
       "      <td>0143110438</td>\n",
       "      <td>R2UFCQ9WES7VFH</td>\n",
       "      <td>1555241537</td>\n",
       "      <td>A great read. In addition to the plot, charact...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A Gentleman in Moscow: A Novel</td>\n",
       "      <td>Knowing nothing ahead of time just walk into t...</td>\n",
       "      <td>D.P. McHenry</td>\n",
       "      <td>0143110438</td>\n",
       "      <td>R24B1HA9J9I99G</td>\n",
       "      <td>1555241542</td>\n",
       "      <td>Great story, well told. Characters were well d...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Pet Sematary: A Novel</td>\n",
       "      <td>One of King's finest, most frightening, &amp; earl...</td>\n",
       "      <td>Gordon Hoffman</td>\n",
       "      <td>198211598X</td>\n",
       "      <td>R1P137WFADSBYR</td>\n",
       "      <td>1555241649</td>\n",
       "      <td>Only the second novel written and published un...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Less (Winner of the Pulitzer Prize): A Novel</td>\n",
       "      <td>Not my favorite</td>\n",
       "      <td>R. Zocher</td>\n",
       "      <td>0316316121</td>\n",
       "      <td>R35533AKR5CBNS</td>\n",
       "      <td>1555242044</td>\n",
       "      <td>This book is t what I expected. Story is a lit...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Supermarket</td>\n",
       "      <td>AMAZING, BOBBY</td>\n",
       "      <td>D. Mahoney</td>\n",
       "      <td>1982127139</td>\n",
       "      <td>R1D6LXSDR0CRMN</td>\n",
       "      <td>1555242169</td>\n",
       "      <td>As someone who hates reading and never really ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X                                    book_title  \\\n",
       "0  1                A Gentleman in Moscow: A Novel   \n",
       "1  2                A Gentleman in Moscow: A Novel   \n",
       "2  3                         Pet Sematary: A Novel   \n",
       "3  4  Less (Winner of the Pulitzer Prize): A Novel   \n",
       "4  5                                   Supermarket   \n",
       "\n",
       "                                        review_title     review_user  \\\n",
       "0  Russian aristocracy following the Russian Revo...      Kansabelle   \n",
       "1  Knowing nothing ahead of time just walk into t...    D.P. McHenry   \n",
       "2  One of King's finest, most frightening, & earl...  Gordon Hoffman   \n",
       "3                                    Not my favorite       R. Zocher   \n",
       "4                                     AMAZING, BOBBY      D. Mahoney   \n",
       "\n",
       "      book_id       review_id   timestamp  \\\n",
       "0  0143110438  R2UFCQ9WES7VFH  1555241537   \n",
       "1  0143110438  R24B1HA9J9I99G  1555241542   \n",
       "2  198211598X  R1P137WFADSBYR  1555241649   \n",
       "3  0316316121  R35533AKR5CBNS  1555242044   \n",
       "4  1982127139  R1D6LXSDR0CRMN  1555242169   \n",
       "\n",
       "                                         review_text  review_score  \n",
       "0  A great read. In addition to the plot, charact...             4  \n",
       "1  Great story, well told. Characters were well d...             4  \n",
       "2  Only the second novel written and published un...             4  \n",
       "3  This book is t what I expected. Story is a lit...             4  \n",
       "4  As someone who hates reading and never really ...             4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data frame in pandas\n",
    "\n",
    "import pandas as pd\n",
    "odf = pd.read_csv('/Users/Gianina/Documents/MSc/spark/notebooks/data.csv', encoding = 'unicode_escape')\n",
    "odf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>book_title</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_user</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A Gentleman in Moscow: A Novel</td>\n",
       "      <td>Russian aristocracy following the Russian Revo...</td>\n",
       "      <td>Kansabelle</td>\n",
       "      <td>0143110438</td>\n",
       "      <td>R2UFCQ9WES7VFH</td>\n",
       "      <td>1555241537</td>\n",
       "      <td>A great read. In addition to the plot, charact...</td>\n",
       "      <td>4</td>\n",
       "      <td>Russian aristocracy following the Russian Revo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A Gentleman in Moscow: A Novel</td>\n",
       "      <td>Knowing nothing ahead of time just walk into t...</td>\n",
       "      <td>D.P. McHenry</td>\n",
       "      <td>0143110438</td>\n",
       "      <td>R24B1HA9J9I99G</td>\n",
       "      <td>1555241542</td>\n",
       "      <td>Great story, well told. Characters were well d...</td>\n",
       "      <td>4</td>\n",
       "      <td>Knowing nothing ahead of time just walk into t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Pet Sematary: A Novel</td>\n",
       "      <td>One of King's finest, most frightening, &amp; earl...</td>\n",
       "      <td>Gordon Hoffman</td>\n",
       "      <td>198211598X</td>\n",
       "      <td>R1P137WFADSBYR</td>\n",
       "      <td>1555241649</td>\n",
       "      <td>Only the second novel written and published un...</td>\n",
       "      <td>4</td>\n",
       "      <td>One of King's finest, most frightening, &amp; earl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Less (Winner of the Pulitzer Prize): A Novel</td>\n",
       "      <td>Not my favorite</td>\n",
       "      <td>R. Zocher</td>\n",
       "      <td>0316316121</td>\n",
       "      <td>R35533AKR5CBNS</td>\n",
       "      <td>1555242044</td>\n",
       "      <td>This book is t what I expected. Story is a lit...</td>\n",
       "      <td>4</td>\n",
       "      <td>Not my favorite  This book is t what I expecte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Supermarket</td>\n",
       "      <td>AMAZING, BOBBY</td>\n",
       "      <td>D. Mahoney</td>\n",
       "      <td>1982127139</td>\n",
       "      <td>R1D6LXSDR0CRMN</td>\n",
       "      <td>1555242169</td>\n",
       "      <td>As someone who hates reading and never really ...</td>\n",
       "      <td>4</td>\n",
       "      <td>AMAZING, BOBBY  As someone who hates reading a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X                                    book_title  \\\n",
       "0  1                A Gentleman in Moscow: A Novel   \n",
       "1  2                A Gentleman in Moscow: A Novel   \n",
       "2  3                         Pet Sematary: A Novel   \n",
       "3  4  Less (Winner of the Pulitzer Prize): A Novel   \n",
       "4  5                                   Supermarket   \n",
       "\n",
       "                                        review_title     review_user  \\\n",
       "0  Russian aristocracy following the Russian Revo...      Kansabelle   \n",
       "1  Knowing nothing ahead of time just walk into t...    D.P. McHenry   \n",
       "2  One of King's finest, most frightening, & earl...  Gordon Hoffman   \n",
       "3                                    Not my favorite       R. Zocher   \n",
       "4                                     AMAZING, BOBBY      D. Mahoney   \n",
       "\n",
       "      book_id       review_id   timestamp  \\\n",
       "0  0143110438  R2UFCQ9WES7VFH  1555241537   \n",
       "1  0143110438  R24B1HA9J9I99G  1555241542   \n",
       "2  198211598X  R1P137WFADSBYR  1555241649   \n",
       "3  0316316121  R35533AKR5CBNS  1555242044   \n",
       "4  1982127139  R1D6LXSDR0CRMN  1555242169   \n",
       "\n",
       "                                         review_text  review_score  \\\n",
       "0  A great read. In addition to the plot, charact...             4   \n",
       "1  Great story, well told. Characters were well d...             4   \n",
       "2  Only the second novel written and published un...             4   \n",
       "3  This book is t what I expected. Story is a lit...             4   \n",
       "4  As someone who hates reading and never really ...             4   \n",
       "\n",
       "                                                text  \n",
       "0  Russian aristocracy following the Russian Revo...  \n",
       "1  Knowing nothing ahead of time just walk into t...  \n",
       "2  One of King's finest, most frightening, & earl...  \n",
       "3  Not my favorite  This book is t what I expecte...  \n",
       "4  AMAZING, BOBBY  As someone who hates reading a...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge review title and text into a single column in pandas\n",
    "odf['text'] = odf.agg('{0[review_title]}  {0[review_text]}'.format, axis=1)\n",
    "odf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------+----------+--------------+----------+--------------------+------------+\n",
      "|  X|          book_title|        review_title|   review_user|   book_id|     review_id| timestamp|         review_text|review_score|\n",
      "+---+--------------------+--------------------+--------------+----------+--------------+----------+--------------------+------------+\n",
      "|  1|A Gentleman in Mo...|Russian aristocra...|    Kansabelle|0143110438|R2UFCQ9WES7VFH|1555241537|A great read. In ...|           4|\n",
      "|  2|A Gentleman in Mo...|Knowing nothing a...|  D.P. McHenry|0143110438|R24B1HA9J9I99G|1555241542|Great story, well...|           4|\n",
      "|  3|Pet Sematary: A N...|One of King's fin...|Gordon Hoffman|198211598X|R1P137WFADSBYR|1555241649|Only the second n...|           4|\n",
      "|  4|Less (Winner of t...|     Not my favorite|     R. Zocher|0316316121|R35533AKR5CBNS|1555242044|This book is t wh...|           4|\n",
      "|  5|         Supermarket|      AMAZING, BOBBY|    D. Mahoney|1982127139|R1D6LXSDR0CRMN|1555242169|As someone who ha...|           4|\n",
      "+---+--------------------+--------------------+--------------+----------+--------------+----------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop_list = ['Dates', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "#data = data.select([column for column in data.columns if column not in drop_list])\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- X: integer (nullable = true)\n",
      " |-- book_title: string (nullable = true)\n",
      " |-- review_title: string (nullable = true)\n",
      " |-- review_user: string (nullable = true)\n",
      " |-- book_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- review_score: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 3236|\n",
      "|           4|  598|\n",
      "|           3|  187|\n",
      "|           1|  110|\n",
      "|           2|  108|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "data.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.count() #this does not work for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+-----------+----------+--------------+----------+-----------+------------+\n",
      "|   X|          book_title|        review_title|review_user|   book_id|     review_id| timestamp|review_text|review_score|\n",
      "+----+--------------------+--------------------+-----------+----------+--------------+----------+-----------+------------+\n",
      "|1121|Lies My Doctor To...|This book gave a ...|   mawshell|162860378X|R3SIH2LVO3EYMH|1555252626|       null|           5|\n",
      "+----+--------------------+--------------------+-----------+----------+--------------+----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#data.filter(\"review_score is NULL\").show() # No nulls anymore\n",
    "#data.filter(\"X is NULL\").show() \n",
    "#data.filter(\"book_title is NULL\").show()\n",
    "#data.filter(\"review_title is NULL\").show()\n",
    "#data.filter(\"review_user is NULL\").show()\n",
    "#data.filter(\"book_id is NULL\").show()\n",
    "#data.filter(\"review_id is NULL\").show()\n",
    "#data.filter(\"timestamp is NULL\").show()\n",
    "data.filter(\"review_text is NULL\").show() # There is one null\n",
    "#data.filter(\"review_score is NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove observation where review_text is null\n",
    "data = data.na.drop(subset=[\"review_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.count() # Removed! #this does not work for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+----------------------------------------------------+-----------+----------+--------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|X  |book_title                    |review_title                                        |review_user|book_id   |review_id     |timestamp |review_text                                                                                                                                                                                                                        |review_score|text                                                                                                                                                                                                                                                                                    |\n",
      "+---+------------------------------+----------------------------------------------------+-----------+----------+--------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1  |A Gentleman in Moscow: A Novel|Russian aristocracy following the Russian Revolution|Kansabelle |0143110438|R2UFCQ9WES7VFH|1555241537|A great read. In addition to the plot, characters, hidden treasure etc. I enjoyed the philosophical question: do Russians destroy more of what they create more than other cultures? Even if you stick to the characters-great fun.|4           |Russian aristocracy following the Russian Revolution A great read. In addition to the plot, characters, hidden treasure etc. I enjoyed the philosophical question: do Russians destroy more of what they create more than other cultures? Even if you stick to the characters-great fun.|\n",
      "+---+------------------------------+----------------------------------------------------+-----------+----------+--------------+----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate book_title, review_title and review_text into a single column\n",
    "from pyspark.sql import functions as ff\n",
    "data = data.withColumn('text', ff.concat(ff.col('review_title'), ff.lit(' '), ff.col('review_text'))) #removed book_title\n",
    "data.show(1, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|review_score|                text|\n",
      "+------------+--------------------+\n",
      "|           4|Russian aristocra...|\n",
      "|           4|Knowing nothing a...|\n",
      "|           4|One of King's fin...|\n",
      "|           4|Not my favorite T...|\n",
      "|           4|AMAZING, BOBBY As...|\n",
      "|           4|A really fun read...|\n",
      "|           4|So truly enjoyabl...|\n",
      "|           4|Well written and ...|\n",
      "|           3|Not one of his be...|\n",
      "|           4|Beautiful It's no...|\n",
      "|           4|The last returns ...|\n",
      "|           4|Don't open this b...|\n",
      "|           4|Bobby Bestseller!...|\n",
      "|           4|Listen to this in...|\n",
      "|           4|A book with a mes...|\n",
      "|           4|Glad I read this ...|\n",
      "|           4|Gentleman From Mo...|\n",
      "|           4|It is a terrific ...|\n",
      "|           2|An OK read if the...|\n",
      "|           4|Just as scary now...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_list = ['X', 'book_title', 'review_title', 'review_user', 'book_id', 'review_id', \n",
    "             'timestamp', 'review_text']\n",
    "data = data.select([column for column in data.columns if column not in drop_list])\n",
    "data.show(20, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Now the modelling pipeline starts</h3>\n",
    "I got it from: https://towardsdatascience.com/multi-class-text-classification-with-pyspark-7d78d022ed35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import nltk\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 3333\n",
      "Test Dataset Count: 905\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "(trainingData, testData) = data.randomSplit([0.8, 0.2], seed = 12345)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expression tokenizer: To split sentences into words\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "stopwordList = nltk.corpus.stopwords.words('english')\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=stopwordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gianina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=20000, minDF=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "# Recoding target variable\n",
    "label_stringIdx = StringIndexer(inputCol = \"review_score\", outputCol = \"label\") \n",
    "labels_stars = label_stringIdx.fit(trainingData).labels # Save this levels to be able later to transform back\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|review_score|                text|               words|            filtered|            features|label|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|           1|*Eye roll* I firs...|[eye, roll, i, fi...|[eye, roll, first...|(415,[0,2,3,4,6,9...|  4.0|\n",
      "|           1|1 of the normal H...|[1, of, the, norm...|[1, normal, start...|(415,[0,16,32,42,...|  4.0|\n",
      "|           1|580 pages to just...|[580, pages, to, ...|[580, pages, quit...|(415,[0,1,2,3,4,5...|  4.0|\n",
      "|           1|A political hit j...|[a, political, hi...|[political, hit, ...|(415,[0,22,54,227...|  4.0|\n",
      "|           1|Amazing Read but ...|[amazing, read, b...|[amazing, read, d...|(415,[0,1,12,25,5...|  4.0|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to training data.\n",
    "pipelineFit = pipeline.fit(trainingData)\n",
    "dataset = pipelineFit.transform(trainingData)\n",
    "#dataset.show(1, truncate = False)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|review_score|                text|               words|            filtered|            features|label|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|           1|A waste of money ...|[a, waste, of, mo...|[waste, money, ti...|(415,[5,10,56,375...|  4.0|\n",
      "|           1|Attn AMAZON Kindl...|[attn, amazon, ki...|[attn, amazon, ki...|(415,[0,11,116,13...|  4.0|\n",
      "|           1|Baid and Switch I...|[baid, and, switc...|[baid, switch, ha...|(415,[21,23,138,2...|  4.0|\n",
      "|           1|Boring ! I'm a hu...|[boring, i, m, a,...|[boring, huge, fa...|(415,[2,22,144,30...|  4.0|\n",
      "|           1|Boring Great plot...|[boring, great, p...|[boring, great, p...|(415,[4,25,63,230...|  4.0|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to test data.\n",
    "test_dataset = pipelineFit.transform(testData)\n",
    "#test_dataset.show(1, truncate = False)\n",
    "test_dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|review_score|count|\n",
      "+------------+-----+\n",
      "|           5| 2543|\n",
      "|           4|  466|\n",
      "|           3|  146|\n",
      "|           2|   90|\n",
      "|           1|   88|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Original encoding\n",
    "trainingData.groupBy(\"review_score\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0| 2543|\n",
      "|  1.0|  466|\n",
      "|  2.0|  146|\n",
      "|  3.0|   90|\n",
      "|  4.0|   88|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show how the encoding changed\n",
    "dataset.groupBy(\"label\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic regression using count vector features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "# Transform back from index to original coding\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedScore\", labels = labels_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------------------------------------------+------------+--------------+\n",
      "|                                         text|                                  probability|review_score|predictedScore|\n",
      "+---------------------------------------------+---------------------------------------------+------------+--------------+\n",
      "|A masterpiece of theory from from a master...|[0.9999999999999991,7.954030889775193E-16,...|           4|             5|\n",
      "|A Mentally Ill Woman, Anna Fox, Witnesses ...|[0.9999999999996585,3.415128690030579E-13,...|           4|             5|\n",
      "|All Stiva, No Lev On the plus side, the no...|[0.9999999999981952,1.1338152616723868E-12...|           4|             5|\n",
      "|exceptional novel - one of the best this y...|[0.9999999999943094,5.690332398125168E-12,...|           5|             5|\n",
      "|Another blockbuster of a historical fictio...|[0.9999999851303771,1.4869556367930017E-8,...|           5|             5|\n",
      "|An engaging (exciting, even) memoir/master...|[0.9999999437499791,3.999597560976421E-8,7...|           5|             5|\n",
      "|Rachel Provides Easy Steps For Creating Ch...|[0.9999996821218458,3.15897100956149E-7,1....|           5|             5|\n",
      "|Bravo, Mr. Towles! After reading \"Rules of...|[0.9999994497352908,5.025798503924341E-7,4...|           5|             5|\n",
      "|Sometimes the price of justice is a good m...|[0.999999206324653,7.936753469304612E-7,7....|           5|             5|\n",
      "|A book with a message for today Growing up...|[0.9999986855974764,1.2894008377595997E-6,...|           4|             5|\n",
      "|Stop \"Dieting\" and Start Living If you are...|[0.9999983228836572,1.5942432237104852E-6,...|           5|             5|\n",
      "|A Must Read! Rich in detail and devastatin...|[0.9999982177613949,1.782238040773086E-6,2...|           5|             5|\n",
      "|A book that stays with you long after you ...|[0.9999981940259043,1.7862692384081904E-6,...|           5|             5|\n",
      "|... beyond five I would choose them endles...|[0.9999975485787979,2.440675650903981E-6,6...|           5|             5|\n",
      "|The best book you will ever read! By follo...|[0.9999958799197344,4.120034622132079E-6,4...|           5|             5|\n",
      "|Finally get to the root cause for female s...|[0.9999957722850747,4.227495921105216E-6,1...|           5|             5|\n",
      "|Beautifully written with elegant Beautiful...|[0.9999955071140441,4.486425082420969E-6,6...|           4|             5|\n",
      "|A fascinating world within a crazy world o...|[0.999991044503695,8.943530639099112E-6,1....|           5|             5|\n",
      "|One of the best books I have read this yea...|[0.9999894421315769,1.0557823991184138E-5,...|           5|             5|\n",
      "|Beautiful story, fascinating time and plac...|[0.9999893580184043,9.105458225896482E-6,1...|           5|             5|\n",
      "+---------------------------------------------+---------------------------------------------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0, elasticNetParam=0)\n",
    "\n",
    "lrModel = lr.fit(dataset) # Fit model\n",
    "predictions = lrModel.transform(test_dataset) # Predict\n",
    "predictions = labelConverter.transform(predictions) # Transform labels\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 20, truncate = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7156069853194736"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions) # This is the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_score=1, text='A waste of money and time Too long... too overly dramatic.  Narrator on audio needs to tone it down down down a few notches. Like waiting for paint to dry.', words=['a', 'waste', 'of', 'money', 'and', 'time', 'too', 'long', 'too', 'overly', 'dramatic', 'narrator', 'on', 'audio', 'needs', 'to', 'tone', 'it', 'down', 'down', 'down', 'a', 'few', 'notches', 'like', 'waiting', 'for', 'paint', 'to', 'dry'], filtered=['waste', 'money', 'time', 'long', 'overly', 'dramatic', 'narrator', 'audio', 'needs', 'tone', 'notches', 'like', 'waiting', 'paint', 'dry'], features=SparseVector(415, {5: 1.0, 10: 1.0, 56: 1.0, 375: 1.0, 395: 1.0}), label=4.0, rawPrediction=DenseVector([-29.7877, -31.5579, -31.4234, -30.8975, -29.5145]), probability=DenseVector([0.3324, 0.0566, 0.0647, 0.1096, 0.4368]), prediction=4.0, predictedScore='1')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Crossvalidation </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7208517548212714"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, regParam=0, elasticNetParam=0)\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.05, 0.1, 0.2, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.2, 0.4, 0.6, 0.8, 1]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "cvModel = cv.fit(dataset)\n",
    "\n",
    "predictions = cvModel.transform(test_dataset)\n",
    "predictions = labelConverter.transform(predictions) # Transform labels\n",
    "\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = LogisticRegression_394821c88d63, numClasses = 5, numFeatures = 415"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = cvModel.bestModel\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "best_reg_param = best_model._java_obj.getRegParam()\n",
    "best_elasticnet_param = best_model._java_obj.getElasticNetParam()\n",
    "print(best_reg_param);print(best_elasticnet_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|A Mentally Ill Woman, Anna Fox, Witne...|[0.999999590727291,4.0898569769996767...|           4|             5|\n",
      "|A masterpiece of theory from from a m...|[0.9999966705537479,2.733524682498626...|           4|             5|\n",
      "|exceptional novel - one of the best t...|[0.9999966412120348,2.139144520826201...|           5|             5|\n",
      "|Sometimes the price of justice is a g...|[0.9999697621108411,2.964081361710977...|           5|             5|\n",
      "|All Stiva, No Lev On the plus side, t...|[0.9999234337172712,8.740677530493783...|           4|             5|\n",
      "|Rachel Provides Easy Steps For Creati...|[0.9999016220931114,6.666044895464801...|           5|             5|\n",
      "|Another blockbuster of a historical f...|[0.9997955036989093,2.02538953473242E...|           5|             5|\n",
      "|The best book you will ever read! By ...|[0.9997196599422713,2.628592920439300...|           5|             5|\n",
      "|Finally get to the root cause for fem...|[0.9995212417266247,4.563821603025323...|           5|             5|\n",
      "|Bravo, Mr. Towles! After reading \"Rul...|[0.9994996677047027,2.247928720608065...|           5|             5|\n",
      "|Stop \"Dieting\" and Start Living If yo...|[0.999435773945948,2.0991916923759272...|           5|             5|\n",
      "|One of the best books I have read thi...|[0.9993000155029443,6.859321793004794...|           5|             5|\n",
      "|A book that stays with you long after...|[0.9992695707744902,5.461194299971187...|           5|             5|\n",
      "|... beyond five I would choose them e...|[0.9991834213788955,7.491806416859094...|           5|             5|\n",
      "|Beautifully written with elegant Beau...|[0.9990337225359014,8.490255641788438...|           4|             5|\n",
      "|An engaging (exciting, even) memoir/m...|[0.9989743649944544,2.581104382575227...|           5|             5|\n",
      "|A fascinating world within a crazy wo...|[0.9988635257256334,9.997681688986544...|           5|             5|\n",
      "|A Must Read! Rich in detail and devas...|[0.9987700504612866,0.001226368074855...|           5|             5|\n",
      "|Learn why your doctor's advice is not...|[0.9986625108027375,6.477055024837301...|           5|             5|\n",
      "|A Gentleman in Moscow is an hilarious...|[0.9984950986720026,0.001495762335590...|           5|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 20, truncate = 40)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predictions.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate and get confusion matrix: https://runawayhorse001.github.io/LearningApacheSpark/classification.html\n",
    "#from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "#metrics = MulticlassMetrics(predictionCol=\"predictions\", labelCol=\"label\")\n",
    "#metrics.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Naive Bayes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|The best book you will ever read! By ...|[0.9999962814618197,3.503214632379414...|           5|             5|\n",
      "|What a POWERFUL story!! Unplanned is ...|[0.9999770160552786,2.298346047933018...|           5|             5|\n",
      "|Beautifully written with elegant Beau...|[0.9999733020487866,2.506506401722169...|           4|             5|\n",
      "|Amazing, beautiful, accessible book! ...|[0.9999701132582072,2.156537250510015...|           5|             5|\n",
      "|Bravo, Mr. Towles! After reading \"Rul...|[0.9999586010516772,1.892012273915955...|           5|             5|\n",
      "|exceptional novel - one of the best t...|[0.9999424971293328,5.642610535653649...|           5|             5|\n",
      "|Rachel Provides Easy Steps For Creati...|[0.999927561543873,6.0412625455744065...|           5|             5|\n",
      "|Anyone ready to change their life thr...|[0.9999228354249022,7.715097289559142...|           5|             5|\n",
      "|A Gentleman in Moscow is an hilarious...|[0.9999224662957438,7.75334640866308E...|           5|             5|\n",
      "|5 stars aren’t Enough! What an absolu...|[0.9999204340077175,7.88800311670668E...|           5|             5|\n",
      "|Stop \"Dieting\" and Start Living If yo...|[0.9999198680094615,4.532133271456414...|           5|             5|\n",
      "|Loved, loved, loved this book This wa...|[0.9999099451362912,8.737743412596169...|           5|             5|\n",
      "|A Must Read What a beautifully writte...|[0.9999054064150894,9.42491676323727E...|           5|             5|\n",
      "|Another beautifully written Amor Towl...|[0.9998515538658097,1.483637678109504...|           5|             5|\n",
      "|Learn why your doctor's advice is not...|[0.9998499745126727,7.974627081584794...|           5|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(smoothing=1)\n",
    "\n",
    "model = nb.fit(dataset)\n",
    "predictions = model.transform(test_dataset)\n",
    "predictions = labelConverter.transform(predictions) # Transform labels\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7323989945784909"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forest</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|Wonderful book!  Must Read! Loved thi...|[0.8020223606805952,0.119060294868325...|           5|             5|\n",
      "|Loved, loved, loved this book This wa...|[0.7968827288093231,0.121055193727271...|           5|             5|\n",
      "|Beautifully narrated. Listened via Au...|[0.7967116264758359,0.121800029620569...|           5|             5|\n",
      "|Loved every minute One of the finest,...|[0.7959696472789387,0.120406497649916...|           5|             5|\n",
      "|Gripping Loved this mystery with a da...|[0.7953760633801066,0.122145671073576...|           5|             5|\n",
      "|first class book arrived on time, bea...|[0.7950154509843793,0.121142918440051...|           4|             5|\n",
      "|A Wonderful and Educational Novel If ...|[0.7949700539183496,0.123231350915792...|           5|             5|\n",
      "|Amazing must read! Great book! I laug...|[0.794097965499896,0.1238971056067009...|           5|             5|\n",
      "|LOVED! Great read! I love Rachel and ...|[0.793410189735031,0.1232176319422132...|           5|             5|\n",
      "|Loved this book! Loved this book! It'...|[0.7932787221671169,0.122594550665354...|           5|             5|\n",
      "|Loved this unique take on the rocksta...|[0.7923458093269974,0.123890093639442...|           5|             5|\n",
      "|Loved it! A very modest account of on...|[0.7923127876624065,0.124795055014586...|           5|             5|\n",
      "|A great read! I loved the history and...|[0.7922501457197932,0.123608218414829...|           5|             5|\n",
      "|Inspiring! Loved this! Inspiring. Gre...|[0.7922501457197932,0.123608218414829...|           5|             5|\n",
      "|Rachel Provides Easy Steps For Creati...|[0.7922319700480327,0.123397736471548...|           5|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(dataset)\n",
    "predictions = rfModel.transform(test_dataset)\n",
    "predictions = labelConverter.transform(predictions) # Transform labels\n",
    "\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6626568462275606"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Crossvalidation </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6697956060200818"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4)\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [100, 200, 500]) # regularization parameter\n",
    "             .addGrid(rf.maxDepth, [4, 10, 20]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "cvModel = cv.fit(dataset)\n",
    "\n",
    "predictions = cvModel.transform(test_dataset)\n",
    "predictions = labelConverter.transform(predictions) # Transform labels\n",
    "\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel (uid=RandomForestClassifier_5586208643d7) with 500 trees"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = cvModel.bestModel\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "best_numTrees = best_model.getNumTrees\n",
    "best_maxDepth = best_model.getOrDefault('maxDepth')\n",
    "print(best_numTrees);print(best_maxDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|Amazing tale and well constructed sto...|[0.9087854836787017,0.060941323855234...|           5|             5|\n",
      "|Wonderful book!  Must Read! Loved thi...|[0.9073353613427794,0.063538289335196...|           5|             5|\n",
      "|One of the best books I have read thi...|[0.906737923225878,0.0626705543419397...|           5|             5|\n",
      "|Amazing must read! Great book! I laug...|[0.903109562062987,0.0563759156793555...|           5|             5|\n",
      "|Best book I have read in a couple of ...|[0.897576428983657,0.0644242280585768...|           5|             5|\n",
      "|Heartwarming. Well written. Lovely bo...|[0.8973500438077334,0.062417050185536...|           5|             5|\n",
      "|Loved every minute One of the finest,...|[0.895349751930963,0.0650061851047260...|           5|             5|\n",
      "|LOVED! Great read! I love Rachel and ...|[0.8948016973180277,0.070631752917829...|           5|             5|\n",
      "|You don't want to miss reading this b...|[0.8946490484202556,0.070611378660704...|           5|             5|\n",
      "|Loved this book! Susannah tells her l...|[0.8922884035198255,0.070715988599067...|           5|             5|\n",
      "|first class book arrived on time, bea...|[0.8921218474206262,0.068289334909087...|           4|             5|\n",
      "|A Wonderful and Educational Novel If ...|[0.8905730637229411,0.069722180781137...|           5|             5|\n",
      "|Absolutely delightful read! This was ...|[0.8901243470627388,0.062813439105308...|           5|             5|\n",
      "|Superb! This is one of the best books...|[0.8897591937745708,0.072952638003676...|           5|             5|\n",
      "|Coben is one of the best Hard to defi...|[0.8890457675299012,0.069064717942932...|           4|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predictions.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TF-IDF Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "pipeline_tfidf = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|review_score|                text|               words|            filtered|         rawFeatures|            features|label|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|           1|*Eye roll* I firs...|[eye, roll, i, fi...|[eye, roll, first...|(20000,[456,1305,...|(20000,[456,1305,...|  4.0|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# applying pipeline to training data\n",
    "pipelineFit_tfidf = pipeline_tfidf.fit(trainingData)\n",
    "dataset_tfidf = pipelineFit_tfidf.transform(trainingData)\n",
    "dataset_tfidf.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|review_score|                text|               words|            filtered|         rawFeatures|            features|label|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|           1|A waste of money ...|[a, waste, of, mo...|[waste, money, ti...|(20000,[437,450,1...|(20000,[437,450,1...|  4.0|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# applying pipeline to test data\n",
    "test_dataset_tfidf = pipelineFit_tfidf.transform(testData)\n",
    "test_dataset_tfidf.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic regression TF-IDF</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|Beautifully done and down to earth I ...|[1.0,1.0951071313748669E-16,5.0335173...|           5|             5|\n",
      "|SO Twisty Twisted “I want my kids to ...|[1.0,6.689446406497144E-17,2.61317220...|           5|             5|\n",
      "|Engrossing story! During the first pa...|[1.0,3.252631500218192E-17,7.17720871...|           5|             5|\n",
      "|SLOW BUILD TO THE TOP OF THE ROLLER C...|[1.0,3.044315421944498E-17,3.72133604...|           5|             5|\n",
      "|On the edge of my seat The author rea...|[1.0,2.4405740959251534E-17,2.8124606...|           5|             5|\n",
      "|Great book! This has been the best bo...|[1.0,1.6857201275336582E-17,4.7220161...|           5|             5|\n",
      "|Can I Give It 10 Stars? What an amazi...|[1.0,1.2134401333156454E-17,1.3185953...|           5|             5|\n",
      "|King at his absolute best. This book ...|[1.0,8.731671491790918E-18,1.16207237...|           5|             5|\n",
      "|Another amazing Black Dagger Brotherh...|[1.0,6.608692984950224E-18,4.17227784...|           5|             5|\n",
      "|A Must Read This is one of the best b...|[1.0,4.164189697349588E-18,4.47423826...|           5|             5|\n",
      "|An Elegant Read - Highly recommended ...|[1.0,3.589723878756446E-18,1.41322591...|           5|             5|\n",
      "|Choose Your Own Adventure! This book ...|[1.0,1.7412393227994232E-18,8.7725755...|           5|             5|\n",
      "|Excellent book This was a well writte...|[1.0,1.6684852576570633E-18,1.2580050...|           5|             5|\n",
      "|I couldn't put it down! Talk about a ...|[1.0,1.5716867169963069E-18,2.2419950...|           5|             5|\n",
      "|Gripping, Consuming This book, with S...|[1.0,1.3784849694631732E-18,7.1969513...|           5|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_tfidf = LogisticRegression(maxIter=20, regParam=0, elasticNetParam=0)\n",
    "\n",
    "lrModel_tfidf = lr_tfidf.fit(dataset_tfidf)\n",
    "predictions_tfidf = lrModel_tfidf.transform(test_dataset_tfidf)\n",
    "predictions_tfidf = labelConverter.transform(predictions_tfidf) # Transform labels\n",
    "\n",
    "predictions_tfidf.filter(predictions_tfidf['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7240072934143862"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_tfidf = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator_tfidf.evaluate(predictions_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_score=1, text='A waste of money and time Too long... too overly dramatic.  Narrator on audio needs to tone it down down down a few notches. Like waiting for paint to dry.', words=['a', 'waste', 'of', 'money', 'and', 'time', 'too', 'long', 'too', 'overly', 'dramatic', 'narrator', 'on', 'audio', 'needs', 'to', 'tone', 'it', 'down', 'down', 'down', 'a', 'few', 'notches', 'like', 'waiting', 'for', 'paint', 'to', 'dry'], filtered=['waste', 'money', 'time', 'long', 'overly', 'dramatic', 'narrator', 'audio', 'needs', 'tone', 'notches', 'like', 'waiting', 'paint', 'dry'], rawFeatures=SparseVector(20000, {437: 1.0, 450: 1.0, 1933: 1.0, 3330: 1.0, 4511: 1.0, 5242: 1.0, 8904: 1.0, 12378: 1.0, 15666: 1.0, 15866: 1.0, 16256: 1.0, 16270: 1.0, 16303: 1.0, 18157: 1.0, 19556: 1.0}), features=SparseVector(20000, {437: 5.547, 450: 4.3507, 1933: 5.9147, 3330: 1.7335, 4511: 4.8538, 5242: 4.1607, 8904: 2.6826, 12378: 5.547, 15666: 3.9688, 15866: 4.6462, 16256: 5.1675, 16270: 0.0, 16303: 0.0, 18157: 1.8222, 19556: 4.5284}), label=4.0, rawPrediction=DenseVector([-2.2326, -3.9938, -2.9213, 6.856, 2.2917]), probability=DenseVector([0.0001, 0.0, 0.0001, 0.9895, 0.0103]), prediction=3.0, predictedScore='2')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_tfidf.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Cross-validation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7321415751587105"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tfidf = LogisticRegression(maxIter=20, regParam=0, elasticNetParam=0)\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr_tfidf.regParam, [0.01, 0.05, 0.1, 0.2, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr_tfidf.elasticNetParam, [0.0, 0.2, 0.4, 0.6, 0.8, 1]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv_tfidf = CrossValidator(estimator=lr_tfidf, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "cvModel_tfidf = cv_tfidf.fit(dataset_tfidf)\n",
    "\n",
    "predictions_tfidf = cvModel_tfidf.transform(test_dataset_tfidf)\n",
    "predictions_tfidf = labelConverter.transform(predictions_tfidf) # Transform labels\n",
    "\n",
    "# Evaluate best model\n",
    "evaluator_tfidf = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator_tfidf.evaluate(predictions_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = LogisticRegression_9dd648d531c8, numClasses = 5, numFeatures = 20000"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_tfidf = cvModel_tfidf.bestModel\n",
    "best_model_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "best_reg_param_tfidf = best_model_tfidf._java_obj.getRegParam()\n",
    "best_elasticnet_param_tfidf = best_model_tfidf._java_obj.getElasticNetParam()\n",
    "print(best_reg_param_tfidf);print(best_elasticnet_param_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|exceptional novel - one of the best t...|[0.9999622696793031,2.733527368071582...|           5|             5|\n",
      "|The best book you will ever read! By ...|[0.9999581685305785,1.678374488334822...|           5|             5|\n",
      "|... beyond five I would choose them e...|[0.9999160145116898,1.548068141767122...|           5|             5|\n",
      "|An engaging (exciting, even) memoir/m...|[0.9997986893320138,2.072211483093863...|           5|             5|\n",
      "|Creative, delicious, and the only cak...|[0.9997223337672339,2.508246223168706...|           5|             5|\n",
      "|Anyone ready to change their life thr...|[0.9997033191053903,2.576619774236201...|           5|             5|\n",
      "|Empowering and Uplifting Let me start...|[0.9996897334926627,1.104777746090736...|           4|             5|\n",
      "|Rachel Provides Easy Steps For Creati...|[0.9996679695897795,1.281634338760052...|           5|             5|\n",
      "|Finally get to the root cause for fem...|[0.9995400540191969,3.154363279797863...|           5|             5|\n",
      "|I mean, it's a classic now, right? Th...|[0.9992489063952487,6.484301444080188...|           5|             5|\n",
      "|Superb Having loved Amor Towles' firs...|[0.9992274282438185,6.931465476376522...|           5|             5|\n",
      "|From the depths of brutality and insa...|[0.9992024405432739,5.738916687801269...|           5|             5|\n",
      "|WOW!  A must read... I read the last ...|[0.9988457024707325,8.98408364729792E...|           5|             5|\n",
      "|A delightful tale of love and ingenui...|[0.9987566034701539,6.03481881191926E...|           5|             5|\n",
      "|Now isn’t it time you step into the E...|[0.9986074668866405,1.754205373886014...|           5|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_tfidf.filter(predictions_tfidf['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predictions_tfidf.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Naive Bayes TF-IDF</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|Holding my breath the whole time! It'...|[1.0,8.91172451906096E-17,9.154199116...|           5|             5|\n",
      "|A classic for sure Amor Towles has wr...|[1.0,8.651849446427587E-17,1.07908278...|           5|             5|\n",
      "|Inspiring and fresh approach Inspirin...|[1.0,7.397583925705214E-17,1.22513161...|           5|             5|\n",
      "|Bye Google! This is a great resource ...|[1.0,5.1578357069290233E-17,5.0991105...|           5|             5|\n",
      "|This is a unique and wonderful story....|[1.0,4.7054284841733154E-17,3.2240275...|           4|             5|\n",
      "|A Favorite I absolutely was entranced...|[1.0,4.3282389009190437E-17,7.1926709...|           5|             5|\n",
      "|Good Story The author takes you throu...|[1.0,3.68905509948127E-17,1.541648874...|           5|             5|\n",
      "|Great idea, great setting, superb wri...|[1.0,3.476216517834812E-17,1.95025841...|           4|             5|\n",
      "|He who changes easiest wins I loved h...|[1.0,3.458381562376753E-17,3.67062433...|           5|             5|\n",
      "|GREG ISLES NEVER DISAPPOINTS! I wait ...|[1.0,3.3019436288188104E-17,8.1053975...|           5|             5|\n",
      "|A book to savor! A book to savor!  Th...|[1.0,2.1762208235408736E-17,6.6585162...|           5|             5|\n",
      "|Highly recommend. Was not expecting t...|[1.0,1.7409071555344027E-17,6.2476749...|           5|             5|\n",
      "|Enchanting from start to end, you won...|[1.0,1.5801398205013542E-17,5.3011346...|           5|             5|\n",
      "|A wonderful story in a unique Russian...|[1.0,1.4599476462207406E-17,6.2174538...|           5|             5|\n",
      "|Love the simplicity! This is the firs...|[1.0,1.3084084524349405E-17,1.0664814...|           5|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_tfidf = NaiveBayes(smoothing=1)\n",
    "\n",
    "model_tfidf = nb_tfidf.fit(dataset_tfidf)\n",
    "predictions_tfidf = model_tfidf.transform(test_dataset_tfidf)\n",
    "predictions_tfidf = labelConverter.transform(predictions_tfidf) # Transform labels\n",
    "\n",
    "predictions_tfidf.filter(predictions_tfidf['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7097232536571314"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_tfidf = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator_tfidf.evaluate(predictions_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forest TF-IDF</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|Can I Give It 10 Stars? What an amazi...|[0.7773785610197007,0.132148672732905...|           5|             5|\n",
      "|One of the best books I have read thi...|[0.7772982631687868,0.131864404793228...|           5|             5|\n",
      "|Five Stars I LOVE this book by Dr Ber...|[0.7772400127825242,0.131678944542849...|           5|             5|\n",
      "|Five Stars I love this book, Dr. Berr...|[0.7772400127825242,0.131678944542849...|           5|             5|\n",
      "|Amazing! A must read! As a mother of ...|[0.7767945295209246,0.132840039242313...|           5|             5|\n",
      "|Amazing must read! Great book! I laug...|[0.7766389580018656,0.132558544803205...|           5|             5|\n",
      "|The Savior OMG...this book is AMAZING...|[0.7761029132706787,0.133001882609265...|           5|             5|\n",
      "|Fantastic! Love it! This book is just...|[0.7761029132706787,0.133001882609265...|           5|             5|\n",
      "|A Must Read What a beautifully writte...|[0.775960146446425,0.1352088810665935...|           5|             5|\n",
      "|Interesting way to tell Russian histo...|[0.775847667136375,0.1325213261404045...|           5|             5|\n",
      "|Another beautifully written Amor Towl...|[0.775659896919363,0.1327256215982261...|           5|             5|\n",
      "|Beautifully written, I wanted more Be...|[0.775659896919363,0.1327256215982261...|           5|             5|\n",
      "|Five Stars Amazing book. The characte...|[0.7755751040942924,0.132976519902215...|           5|             5|\n",
      "|Five Stars Love Stephen King. This on...|[0.7754436566284477,0.132717560915331...|           5|             5|\n",
      "|Five Stars Fabulous book if you love ...|[0.7754436566284477,0.132717560915331...|           5|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf_tfidf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel_tfidf = rf_tfidf.fit(dataset_tfidf)\n",
    "predictions_tfidf = rfModel_tfidf.transform(test_dataset_tfidf)\n",
    "predictions_tfidf = labelConverter.transform(predictions_tfidf) # Transform labels\n",
    "\n",
    "predictions_tfidf.filter(predictions_tfidf['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6626568462275606"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_tfidf = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator_tfidf.evaluate(predictions_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Crossvalidation </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6652653733781968"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "rf_tfidf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4)\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf_tfidf.numTrees, [100, 200, 500]) # regularization parameter\n",
    "             .addGrid(rf_tfidf.maxDepth, [4, 10, 20]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv_tfidf = CrossValidator(estimator=rf_tfidf, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "cvModel_tfidf = cv_tfidf.fit(dataset_tfidf)\n",
    "\n",
    "predictions_tfidf = cvModel_tfidf.transform(test_dataset_tfidf)\n",
    "predictions_tfidf = labelConverter.transform(predictions_tfidf) # Transform labels\n",
    "\n",
    "# Evaluate best model\n",
    "evaluator_tfidf = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "evaluator_tfidf.evaluate(predictions_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel (uid=RandomForestClassifier_351e11f29ab0) with 500 trees"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_tfidf = cvModel_tfidf.bestModel\n",
    "best_model_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "best_numTrees_tfidf = best_model_tfidf.getNumTrees\n",
    "best_maxDepth_tfidf = best_model_tfidf.getOrDefault('maxDepth')\n",
    "print(best_numTrees_tfidf);print(best_maxDepth_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|Amazing must read! Great book! I laug...|[0.8394878420017327,0.098090323020065...|           5|             5|\n",
      "|Wonderful book!  Must Read! Loved thi...|[0.8350624376964675,0.103200703017782...|           5|             5|\n",
      "|Amazing tale and well constructed sto...|[0.8340589625245075,0.101695406571931...|           5|             5|\n",
      "|One of the best books I have read thi...|[0.8300513716897474,0.105026777976361...|           5|             5|\n",
      "|Loved, loved, loved this book This wa...|[0.8277551885868162,0.106696788690534...|           5|             5|\n",
      "|Loved this book! Loved this book! It'...|[0.8261263650670962,0.105873147661166...|           5|             5|\n",
      "|5 stars aren’t Enough! What an absolu...|[0.8257218220105224,0.103640884757110...|           5|             5|\n",
      "|A Wonderful and Educational Novel If ...|[0.8257086126975356,0.107933883931365...|           5|             5|\n",
      "|Loved this book! Susannah tells her l...|[0.8255632632667492,0.106719056498556...|           5|             5|\n",
      "|first class book arrived on time, bea...|[0.8247208295268611,0.106099841718886...|           4|             5|\n",
      "|Great book! This has been the best bo...|[0.8244945746446438,0.102499994001282...|           5|             5|\n",
      "|Awesome Novel - Not to be Missed Best...|[0.8239044216974601,0.106662693652428...|           5|             5|\n",
      "|Excellent well written book about an ...|[0.8237734723093199,0.109302444791946...|           5|             5|\n",
      "|Heartwarming. Well written. Lovely bo...|[0.8230847563416945,0.106904419365415...|           5|             5|\n",
      "|LOVED! Great read! I love Rachel and ...|[0.822945875873666,0.1083256060359850...|           5|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_tfidf.filter(predictions_tfidf['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predictions_tfidf.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Word2Vec </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "w2v = Word2Vec(vectorSize=3, minCount=0, inputCol=\"filtered\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_w2v = Pipeline(stages=[regexTokenizer, stopwordsRemover, w2v, label_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|review_score|                text|               words|            filtered|            features|label|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|           1|*Eye roll* I firs...|[eye, roll, i, fi...|[eye, roll, first...|[0.12171184380861...|  4.0|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# applying pipeline to training data\n",
    "pipelineFit_w2v = pipeline_w2v.fit(trainingData)\n",
    "dataset_w2v = pipelineFit_w2v.transform(trainingData)\n",
    "dataset_w2v.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|review_score|                text|               words|            filtered|            features|label|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|           1|A waste of money ...|[a, waste, of, mo...|[waste, money, ti...|[0.12965251902739...|  4.0|\n",
      "+------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# applying pipeline to test data\n",
    "test_dataset_w2v = pipelineFit_w2v.transform(testData)\n",
    "test_dataset_w2v.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic regression Word2Vec</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                        Five Stars GREAT|[0.9990179946518062,8.926592373259777...|           5|             5|\n",
      "|                      Five Stars Thanks!|[0.9989624988812209,9.030248418269357...|           5|             5|\n",
      "|                     Five Stars loved it|[0.9988892388307916,0.001030702828146...|           5|             5|\n",
      "|                    Five Stars good book|[0.9938376445727559,0.004656102111888...|           5|             5|\n",
      "|Five Stars Informative and great writing|[0.9916983515107264,0.007132063323189...|           5|             5|\n",
      "|Five Stars It doesn't get any better ...|[0.9915531519304558,0.006641721701242...|           5|             5|\n",
      "|               Five Stars Cute and short|[0.9906646786105789,0.007282884570888...|           5|             5|\n",
      "|  Five Stars Classic that I finally read|[0.9903446252368877,0.007833918099777...|           5|             5|\n",
      "|       Five Stars I really like Dr Beery|[0.9892877150700621,0.007272007409325...|           5|             5|\n",
      "|Five Stars Excellent!! A real page tu...|[0.98822502154914,0.01028991631155213...|           5|             5|\n",
      "|Five Stars Recipes are great and easy...|[0.987968625089786,0.0092978958320203...|           5|             5|\n",
      "|Five Stars Recipes are great and easy...|[0.987968625089786,0.0092978958320203...|           5|             5|\n",
      "|Five Stars I LOVE this book by Dr Ber...|[0.9845878273453817,0.009112630349884...|           5|             5|\n",
      "|Five Stars I love this book, Dr. Berr...|[0.9841590604834575,0.010297951248578...|           5|             5|\n",
      "|Five Stars Best book I've read in 201...|[0.9834026158935099,0.012480830964334...|           5|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_w2v = LogisticRegression(maxIter=20, regParam=0, elasticNetParam=0)\n",
    "\n",
    "lrModel_w2v = lr_w2v.fit(dataset_w2v)\n",
    "predictions_w2v = lrModel_w2v.transform(test_dataset_w2v)\n",
    "predictions_w2v = labelConverter.transform(predictions_w2v) # Transform labels\n",
    "\n",
    "predictions_w2v.filter(predictions_w2v['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_score=1, text='A waste of money and time Too long... too overly dramatic.  Narrator on audio needs to tone it down down down a few notches. Like waiting for paint to dry.', words=['a', 'waste', 'of', 'money', 'and', 'time', 'too', 'long', 'too', 'overly', 'dramatic', 'narrator', 'on', 'audio', 'needs', 'to', 'tone', 'it', 'down', 'down', 'down', 'a', 'few', 'notches', 'like', 'waiting', 'for', 'paint', 'to', 'dry'], filtered=['waste', 'money', 'time', 'long', 'overly', 'dramatic', 'narrator', 'audio', 'needs', 'tone', 'notches', 'like', 'waiting', 'paint', 'dry'], features=DenseVector([0.1297, -0.0, -0.1009]), label=4.0, rawPrediction=DenseVector([2.2142, 0.4116, -0.6994, -1.0434, -0.883]), probability=DenseVector([0.7676, 0.1266, 0.0417, 0.0295, 0.0347]), prediction=0.0, predictedScore='5')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_w2v.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6621138481563025"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_w2v = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator_w2v.evaluate(predictions_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Cross-validation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6626568462275606"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_w2v = LogisticRegression(maxIter=20, regParam=0, elasticNetParam=0)\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr_w2v.regParam, [0.01, 0.05, 0.1, 0.2, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr_w2v.elasticNetParam, [0.0, 0.2, 0.4, 0.6, 0.8, 1]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv_w2v = CrossValidator(estimator=lr_w2v, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "cvModel_w2v = cv_w2v.fit(dataset_w2v)\n",
    "\n",
    "predictions_w2v = cvModel_w2v.transform(test_dataset_w2v)\n",
    "predictions_w2v = labelConverter.transform(predictions_w2v) # Transform labels\n",
    "\n",
    "# Evaluate best model\n",
    "evaluator_w2v = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator_w2v.evaluate(predictions_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = LogisticRegression_962370e30246, numClasses = 5, numFeatures = 3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_w2v = cvModel_w2v.bestModel\n",
    "best_model_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "best_reg_param_w2v = best_model_w2v._java_obj.getRegParam()\n",
    "best_elasticnet_param_w2v = best_model_w2v._java_obj.getElasticNetParam()\n",
    "print(best_reg_param_w2v);print(best_elasticnet_param_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                        Five Stars GREAT|[0.9966192352581656,0.002631490546815...|           5|             5|\n",
      "|                     Five Stars loved it|[0.9963680578485298,0.002971471338022...|           5|             5|\n",
      "|                      Five Stars Thanks!|[0.9962178764782932,0.002683675545206...|           5|             5|\n",
      "|                    Five Stars good book|[0.9857984657457295,0.008851949287828...|           5|             5|\n",
      "|Five Stars It doesn't get any better ...|[0.9814926845062876,0.012268472772541...|           5|             5|\n",
      "|Five Stars Informative and great writing|[0.9813553638420999,0.013978208821741...|           5|             5|\n",
      "|  Five Stars Classic that I finally read|[0.9793110506408935,0.014365840986768...|           5|             5|\n",
      "|               Five Stars Cute and short|[0.9786605573568864,0.013988621006381...|           5|             5|\n",
      "|       Five Stars I really like Dr Beery|[0.9761739547979577,0.013416632082582...|           5|             5|\n",
      "|Five Stars Excellent!! A real page tu...|[0.9757879909293836,0.018838436894244...|           5|             5|\n",
      "|Five Stars Recipes are great and easy...|[0.9747926393268823,0.016555173066531...|           5|             5|\n",
      "|Five Stars Recipes are great and easy...|[0.9747926393268823,0.016555173066531...|           5|             5|\n",
      "|Five Stars I LOVE this book by Dr Ber...|[0.9702331391765079,0.015113519355240...|           5|             5|\n",
      "|Five Stars Best book I've read in 201...|[0.9694842949389475,0.019763914088979...|           5|             5|\n",
      "|Five Stars I love this book, Dr. Berr...|[0.96797814503912,0.01763401414124400...|           5|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_w2v.filter(predictions_w2v['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predictions_w2v.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Naive Bayes Word2Vec</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o49027.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8400.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8400.0 (TID 8996, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [0.12171184380861873,-0.0021886859755554443,-0.12637901212069755].\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\r\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\r\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:176)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:129)\r\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:129)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:118)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:78)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\r\n\tat sun.reflect.GeneratedMethodAccessor252.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [0.12171184380861873,-0.0021886859755554443,-0.12637901212069755].\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\r\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\r\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-298efa21db94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnb_w2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmoothing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_w2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_w2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpredictions_w2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_w2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpredictions_w2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabelConverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_w2v\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Transform labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MSc\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32m~\\Documents\\MSc\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MSc\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \"\"\"\n\u001b[0;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MSc\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MSc\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MSc\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o49027.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8400.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8400.0 (TID 8996, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [0.12171184380861873,-0.0021886859755554443,-0.12637901212069755].\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\r\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\r\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:176)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:129)\r\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\r\n\tat scala.util.Try$.apply(Try.scala:192)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:129)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:118)\r\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:78)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\r\n\tat sun.reflect.GeneratedMethodAccessor252.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: requirement failed: Naive Bayes requires nonnegative feature values but found [0.12171184380861873,-0.0021886859755554443,-0.12637901212069755].\r\n\tat scala.Predef$.require(Predef.scala:224)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$.requireNonnegativeValues(NaiveBayes.scala:235)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$4.apply(NaiveBayes.scala:144)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:168)\r\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1$$anonfun$7.apply(NaiveBayes.scala:166)\r\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$aggregateByKey$1$$anonfun$apply$6.apply(PairRDDFunctions.scala:172)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:189)\r\n\tat org.apache.spark.util.collection.ExternalSorter$$anonfun$5.apply(ExternalSorter.scala:188)\r\n\tat org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:144)\r\n\tat org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)\r\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:194)\r\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "nb_w2v = NaiveBayes(smoothing=1)\n",
    "\n",
    "model_w2v = nb_w2v.fit(dataset_w2v)\n",
    "predictions_w2v = model_w2v.transform(test_dataset_w2v)\n",
    "predictions_w2v = labelConverter.transform(predictions_w2v) # Transform labels\n",
    "\n",
    "predictions_w2v.filter(predictions_w2v['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator_w2v = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator_w2v.evaluate(predictions_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forest Word2Vec</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|Five Stars Recipes are great and easy...|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|                     Five Stars loved it|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|                      Five Stars Thanks!|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|                        Five Stars GREAT|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|Five Stars Informative and great writing|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|Five Stars It doesn't get any better ...|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|Five Stars Recipes are great and easy...|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|Five Stars Best book I've read in 201...|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|                    Five Stars good book|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|  Five Stars Classic that I finally read|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|Five Stars Excellent!! A real page tu...|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|    Stephen King Books Love Stephen King|[0.8554654852906513,0.087835100409996...|           5|             5|\n",
      "|Five Stars A great book, suspenseful ...|[0.8547543688842355,0.087729139201750...|           5|             5|\n",
      "|Five Stars Love Stephen King. This on...|[0.8547543688842355,0.087729139201750...|           5|             5|\n",
      "|               Five Stars Cute and short|[0.8547543688842355,0.087729139201750...|           5|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_w2v = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "\n",
    "# Train model with Training Data\n",
    "rfModel_w2v = rf_w2v.fit(dataset_w2v)\n",
    "predictions_w2v = rfModel_w2v.transform(test_dataset_w2v)\n",
    "predictions_w2v = labelConverter.transform(predictions_w2v) # Transform labels\n",
    "\n",
    "predictions_w2v.filter(predictions_w2v['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6626568462275606"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_w2v = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator_w2v.evaluate(predictions_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Crossvalidation </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6626568462275606"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_w2v = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4)\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf_w2v.numTrees, [100, 200, 500]) # regularization parameter\n",
    "             .addGrid(rf_w2v.maxDepth, [4, 10, 20]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv_w2v = CrossValidator(estimator=rf_w2v, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "\n",
    "cvModel_w2v = cv_w2v.fit(dataset_w2v)\n",
    "\n",
    "predictions_w2v = cvModel_w2v.transform(test_dataset_w2v)\n",
    "predictions_w2v = labelConverter.transform(predictions_w2v) # Transform labels\n",
    "\n",
    "# Evaluate best model\n",
    "evaluator_w2v = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator_w2v.evaluate(predictions_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel (uid=RandomForestClassifier_37eb3a9816a4) with 100 trees"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_w2v = cvModel_w2v.bestModel\n",
    "best_model_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "best_numTrees_w2v = best_model_w2v.getNumTrees\n",
    "best_maxDepth_w2v = best_model_w2v.getOrDefault('maxDepth')\n",
    "print(best_numTrees_w2v);print(best_maxDepth_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|                                    text|                             probability|review_score|predictedScore|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "|Five Stars Recipes are great and easy...|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|                     Five Stars loved it|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|                      Five Stars Thanks!|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|                        Five Stars GREAT|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|Five Stars Informative and great writing|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|Five Stars It doesn't get any better ...|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|Five Stars Recipes are great and easy...|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|Five Stars Best book I've read in 201...|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|                    Five Stars good book|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|  Five Stars Classic that I finally read|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|Five Stars Excellent!! A real page tu...|[0.8571389889976624,0.086616307675158...|           5|             5|\n",
      "|    Stephen King Books Love Stephen King|[0.8554654852906513,0.087835100409996...|           5|             5|\n",
      "|Five Stars A great book, suspenseful ...|[0.8547543688842355,0.087729139201750...|           5|             5|\n",
      "|Five Stars Love Stephen King. This on...|[0.8547543688842355,0.087729139201750...|           5|             5|\n",
      "|               Five Stars Cute and short|[0.8547543688842355,0.087729139201750...|           5|             5|\n",
      "+----------------------------------------+----------------------------------------+------------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_w2v.filter(predictions_w2v['prediction'] == 0) \\\n",
    "    .select(\"text\",\"probability\",\"review_score\",\"predictedScore\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 15, truncate = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_score=1, text='A waste of money and time Too long... too overly dramatic.  Narrator on audio needs to tone it down down down a few notches. Like waiting for paint to dry.', words=['a', 'waste', 'of', 'money', 'and', 'time', 'too', 'long', 'too', 'overly', 'dramatic', 'narrator', 'on', 'audio', 'needs', 'to', 'tone', 'it', 'down', 'down', 'down', 'a', 'few', 'notches', 'like', 'waiting', 'for', 'paint', 'to', 'dry'], filtered=['waste', 'money', 'time', 'long', 'overly', 'dramatic', 'narrator', 'audio', 'needs', 'tone', 'notches', 'like', 'waiting', 'paint', 'dry'], features=DenseVector([0.1297, -0.0, -0.1009]), label=4.0, rawPrediction=DenseVector([73.4686, 15.3011, 4.9771, 3.1385, 3.1147]), probability=DenseVector([0.7347, 0.153, 0.0498, 0.0314, 0.0311]), prediction=0.0, predictedScore='5')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_w2v.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.66666599999999,
   "position": {
    "height": "144px",
    "left": "1009px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
